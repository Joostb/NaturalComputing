\documentclass[11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{multicol}
\usepackage{hyperref}
\epstopdfsetup{outdir=./images/}
\usepackage{subcaption}
\usepackage[table,xcdraw]{xcolor}

\renewcommand{\thesubsection}{(\alph{subsection})}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\title{Natural Computing, Assignment 3}
\author{Dennis Verheijden - s4455770 \and Pauline Lauron - s1016609 \and Joost Besseling - s4796799}
\begin{document}
\maketitle

\section*{Combining, Bagging \& Random Forests}
\section{}
\subsection{}
\begin{itemize}
	\item 
	The probability that all three doctors give the correct answer is $ 0.8^3 = 0.512$. 
	\item 
	The probability that exactly 2 doctors make the right call is $0.8*0.8*0.2 + 0.8*0.2*0.8 + 0.2*0.8*0.8 = 0.384$. Therefore, the probability that \emph{at least} two doctors make the right call is $0.512 + 0.384 = 0.896$.
	\item  The probability that this group makes the right decision based on majority voting is $0.512 + 0.384 = 0.896$ since the majority is when there are at least two individuals. 
	%(Alternative approach, the probability that they all fail, and that exactly two doctors fail is ...)
\end{itemize}


\subsection{}
The general formula is 
\[
	P(\text{correct predictions} > c/2) \sum_{i = \floor{n/2}}^{n} p^{i}  (1-p)^{n-i}  \binom{n}{i}.
\]

Using this formula, we find a probability of about $0.826$.

\subsection{}
If we use $10000$ runs of the simulations, we get an approximately equal result of $0.826$. Writing out more decimals gives us a difference of $0.0057$. So our approximation is pretty good.

\subsection{}
We decided to use a surfplot for the visualization. Here we can easily spot the differences when variables change relative to each other.

The surfplot can be found in figure \ref{fig:surf_probs}. What we can observe from this plot is that the jury size matters for a low number of people, but that this effect has exponentially diminishing returns. The competence, as expected, has the highest effect on the probability of correctly making the right decision.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{images/surfplot_probs.eps}
	\caption{Surfplot of the probabilities as a function of the jury size $c$ and competence $p$.}
	\label{fig:surf_probs}
\end{figure}

\subsection{}
The probabilities for making the correct decision for the groups are:
\begin{itemize}
	\item \textbf{radiologists}: $0.850$
	\item \textbf{doctors}: $0.896$
	\item \textbf{students}: $0.826$
\end{itemize}
To reach the same probability for making the correct decision as the group of doctors, using only students. You would need 28 students. These would, collectively, have a probability of making the correct decision of $0.898$.

\section{}
The filled in table may be found below in table \ref{table:a}. Here we can see the values for different combinators.  Colors denote the choice that the classifier will make. What we can note from this table is that the choices of these classifiers are in most cases the same. 
\begin{table}[H]
	\centering
	\centering
	\begin{tabular}{ll|ll|ll|ll|ll|ll|ll}
		\multicolumn{2}{l}{$p_1(\omega|x)$} & \multicolumn{2}{l|}{$p_2(\omega|x)$} & \multicolumn{2}{l|}{$p_3(\omega|x)$} & \multicolumn{2}{l|}{Mean}                                 & \multicolumn{2}{l|}{Max}                                  & \multicolumn{2}{l|}{Min}                                  & \multicolumn{2}{l|}{Prod}                                     \\ \hline
		A                & B                & A                 & B                & A                 & B                & A                           & B                           & A                           & B                           & A                           & B                           & A                             & B                             \\ \hline
		0.9              & 0.1              & 0.9               & 0.1              & 0.0               & 1.0              & \cellcolor[HTML]{F8A102}0.6 & 0.4                         & 0.9                         & \cellcolor[HTML]{F8A102}1.0 & 0.0                         & \cellcolor[HTML]{F8A102}0.1 & 0.0                           & \cellcolor[HTML]{F8A102}0.01  \\
		0.9              & 0.1              & 0.9               & 0.1              & 0.3               & 0.7              & \cellcolor[HTML]{F8A102}0.7 & 0.3                         & \cellcolor[HTML]{F8A102}0.9 & 0.7                         & \cellcolor[HTML]{F8A102}0.3 & 0.1                         & \cellcolor[HTML]{F8A102}0.243 & 0.007                         \\
		0.9              & 0.1              & 0.2               & 0.8              & 0.1               & 0.9              & 0.4                         & \cellcolor[HTML]{F8A102}0.6 & \cellcolor[HTML]{FD6864}0.9 & \cellcolor[HTML]{FD6864}0.9 & \cellcolor[HTML]{FD6864}0.1 & \cellcolor[HTML]{FD6864}0.1 & 0.018                         & \cellcolor[HTML]{F8A102}0.072 \\
		0.0              & 1.0              & 0.0               & 1.0              & 0.0               & 1.0              & 0.0                         & \cellcolor[HTML]{F8A102}1.0 & 0.0                         & \cellcolor[HTML]{F8A102}1.0 & 0.0                         & \cellcolor[HTML]{F8A102}1.0 & 0.0                           & \cellcolor[HTML]{F8A102}1.0  
	\end{tabular}
	\caption{Classifier Combination for different methods. Values indicated in orange denote the decision the combiner would make. Values indicated in red denote a tie, the choice here is based on the implementation.}
	\label{table:a}
\end{table}

\section{}
The formula for the percentage of items that are not in a sample using Bootstrapping is:
\[
	p_{notPresent} = (1 - \frac{1}{N})^N
\]
We also made simulations which approximate this function, as found in figure \ref{fig:approx_vs_real}. Here we see that our simulations approximate our formula. Which adds evidence to its correctness.

Now that we have this implementation, we can also make simulations for different $N$. The results for this are shown in figure \ref{fig:leftout}. Here we can see the comparisons for different $N$. We see that the probability of items that do not occur converge to $0.37$ for larger $N$. We see that there probably is an exponential relation as the probability first rapidly increases to $0.25$ for $N = 2$ and at $N=5$ it is almost at convergence.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{images/approx_vs_real.eps}
	\caption{Comparison of our simulation and the exact value.}
	\label{fig:approx_vs_real}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{images/different_sample_sizes.eps}
	\caption{Comparison of different sample sizes for the percentage of left-out items.}
	\label{fig:leftout}
\end{figure}

\section{}
The difference between bootstrapping and random subspaces is how the training data is constructed. 
\begin{itemize}
	\item In bootstrapping, you randomly select data from the whole dataset with replacement. Such that you can have samples that occur more than once.
	\item In random subspaces, you randomly select features from the whole dataset. So instead of randomly drawing samples, you draw features from these samples.
\end{itemize}


\section{}
For random forests, the importance of a variable is calculated using a criterion. The most used measure in Forest Classifiers is the Gini Impurity. In short, the Gini Impurity looks for every feature, if we would split based on this feature, how pure the split would be, i.e. how well it separates the classes. The main drawback of this approach is that these classifiers are extremely sensitive to small changes in your training set. One slight change could create a drastically different tree.
% the Mean Decrease Impurity. This compute how much each feature decreases the weighted impurity in a tree where the impurity is the measure based on which the optimal condition is chosen.
%critics : http://blog.datadive.net/selecting-good-features-part-iii-random-forests/

\section{}
NOTE: What is the baseline? It is 1/10th right?

~\\
For this experiment we used the handwritten digits, aka MNIST, dataset. As this dataset is easy to interpret and it has a decent difficulty to learn as the dataset contains 64 features and 10 classes.

We used the standard \texttt{RandomForestClassifier} from the python module \texttt{sklearn}. We found two variables that we thought to be important: \textit{max\_depth}, \textit{n\_estimators} and \textit{max\_features}. These respectively mean the maximum depth of a tree, the number of trees that are trained in the forest and the maximum number of features Random Forest is allowed to try in individual tree. The result of manipulating the first two parameters are found in figure \ref{fig:mnist_results}.

What we can note from this figure, is that both parameters converge. The default value for \textit{n\_estimators} is $10$. This value is roughly the value for which the algorithm converges. The default value for \textit{max\_depth} is not set, i.e. it expands until all leaves are pure or contain less than 2 samples. To better see whether setting this value is helpful, we can examine the results for certain values of the \textit{max\_depth} while leaving the parameter \textit{n\_estimators} to the default value. The results may be found in figure \ref{fig:mnist_lines}. What we can observe is that for a max tree depth of $1$ or $2$ the RandomForestClassifier is already way above chance, being $0.1$. The performance peaks at a max tree depth of 10. After this it starts to decline. This is because the tree is overfitting on the training set, which is an easy pitfall of this type of classifier. However, since it is an ensemble, the damage of overfitting is limited.

~\\ \textbf{NOTE: The part about manipulating \textit{max\_features} should be more fleshed out} \\
To test different combination of parameters, we use the Grid Search. We obtain a score around 0.93.
~\\

Conclusion, setting the parameter \textit{n\_estimators} might not be necessary, depending on the domain. However, more estimators lead to more stable predictions. The parameter \textit{max\_depth} however, should be carefully set, as it has the most effect on the mean accuracy of the RandomForestClassifier.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/mnist_results.eps}
\caption{The mean accuracy of RandomForstClassifiers for different parameter setups.}
\label{fig:mnist_results}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/mnist_lines.eps}
\caption{The mean accuracy of RandomForstClassifiers for choices for the max depth of the trees. The number of trees is set to the default value of $10$. A tree depth of \textit{None} means that the tree is allowed to split until the leaf nodes are either pure or have less than 2 samples.}
\label{fig:mnist_lines}
\end{figure}

\section*{Boosting}

\section*{1}


\section*{2}

\section*{3}

In bagging we train multiple learners by sampling from the dataset with replacement. We combine them by, for example, taking the average decision, or using a majority vote.

In boosting, we also train multiple learners. But we use the learners to weight the samples from the dataset, such that samples that we predict wrong early on, get a higher wait. We combine the learners similarly to the boosting case, but we weight them according to their results on the training set.

\section*{4}

\section*{5}


\end{document}